"""
web_app.py

Interface web compl√®te pour le syst√®me de trading SAC EUR/USD.
Fournit un dashboard temps r√©el, gestion des mod√®les, configuration et monitoring.

Features:
- Dashboard temps r√©el avec SocketIO
- REST API pour contr√¥le du syst√®me
- Gestion des mod√®les (load, train, retrain)
- √âditeur de configuration
- Visualisation des performances
- Monitoring des trades live

Author: Cl√©ment
Version: 1.0
"""

import os
import sys
import json
import yaml
import logging
import threading
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any
import traceback

import numpy as np
import pandas as pd
import torch
from flask import Flask, render_template, request, jsonify, send_from_directory
from flask_socketio import SocketIO, emit
from flask_cors import CORS
import plotly
import plotly.graph_objs as go

# Import modules du syst√®me
from backend.data_pipeline import DataPipeline
from backend.feature_engineering import FeatureEngineer
from backend.hmm_detector import HMMRegimeDetector
from backend.trading_env import TradingEnvironment
from backend.sac_agent import SACAgent, SACConfig
from backend.auxiliary_task import AuxiliaryTaskLearner
from backend.ensemble_meta import EnsembleMetaController
from backend.validation import ValidationFramework
from backend.mt5_connector import MT5Connector
from backend.risk_manager import RiskManager

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/web_app.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

BASE_DIR = Path(__file__).resolve().parent
FRONTEND_DIR = BASE_DIR.parent / "frontend"
TEMPLATE_DIR = FRONTEND_DIR / "templates"
STATIC_DIR = FRONTEND_DIR / "static"

# Configuration Flask
app = Flask(
    __name__,
    template_folder=str(TEMPLATE_DIR),
    static_folder=str(STATIC_DIR)
)
app.config['SECRET_KEY'] = 'your-secret-key-change-in-production'
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max upload
CORS(app)

# Configuration SocketIO
socketio = SocketIO(app, cors_allowed_origins="*", async_mode='threading')

# Variables globales pour le syst√®me
class TradingSystemState:
    """√âtat global du syst√®me de trading"""
    def __init__(self):
        self.is_training = False
        self.is_trading = False
        self.current_model = None
        self.ensemble_controller = None
        self.mt5_connector = None
        self.risk_manager = None
        self.data_pipeline = None
        self.feature_engineer = None
        self.hmm_detector = None

        # M√©triques en temps r√©el
        self.current_equity = 10000.0
        self.current_position = 0.0
        self.current_pnl = 0.0
        self.daily_trades = []
        self.performance_metrics = {}

        # Historique
        self.equity_history = []
        self.trade_history = []
        self.signal_history = []

        # Configuration
        self.config = self.load_config()

        # Threading
        self.trading_thread = None
        self.monitoring_thread = None
        self.stop_event = threading.Event()

        # √âtat du training
        self.training_state = self.load_training_state()
        
    def load_config(self) -> Dict:
        """Charge la configuration depuis YAML"""
        config_path = Path('config/system_config.yaml')
        if config_path.exists():
            with open(config_path, 'r') as f:
                return yaml.safe_load(f)
        else:
            # Configuration par d√©faut
            return {
                'data': {
                    'data_dir': 'data',
                    'timeframe': '5min',
                    'pairs': ['EURUSD', 'USDJPY', 'GBPUSD', 'USDCAD']
                },
                'trading': {
                    'initial_capital': 500000.0,
                    'risk_per_trade': 0.05,
                    'max_daily_loss': 0.05,
                    'max_drawdown': 0.15
                },
                'model': {
                    'checkpoint_dir': 'models/checkpoints',
                    'production_dir': 'models/production',
                    'ensemble_size': 3
                },
                'mt5': {
                    'server': 'ICMarketsEU-Demo',
                    'login': 0,
                    'password': '',
                    'symbol': 'EURUSD'
                }
            }
    
    def save_config(self):
        """Sauvegarde la configuration"""
        config_path = Path('config/system_config.yaml')
        config_path.parent.mkdir(parents=True, exist_ok=True)
        with open(config_path, 'w') as f:
            yaml.dump(self.config, f)

    def load_training_state(self) -> Dict:
        """Charge l'√©tat du training depuis JSON"""
        state_path = Path('logs/training_state.json')
        if state_path.exists():
            with open(state_path, 'r') as f:
                return json.load(f)
        return {
            'is_training': False,
            'current_agent': None,
            'current_episode': 0,
            'total_episodes': 0,
            'start_time': None,
            'metrics': {},
            'training_type': None  # 'sac_agent' or 'meta_controller'
        }

    def save_training_state(self):
        """Sauvegarde l'√©tat du training"""
        state_path = Path('logs/training_state.json')
        state_path.parent.mkdir(parents=True, exist_ok=True)
        with open(state_path, 'w') as f:
            json.dump(self.training_state, f, indent=2)

# √âtat global
system_state = TradingSystemState()

# ============================================================================
# ROUTES WEB INTERFACE
# ============================================================================

@app.route('/')
def index():
    """Page principale du dashboard"""
    return render_template('index.html')

@app.route('/models')
def models_page():
    """Page de gestion des mod√®les"""
    return render_template('models.html')

@app.route('/config')
def config_page():
    """Page de configuration"""
    return render_template('config.html')

@app.route('/validation')
def validation_page():
    """Page de validation et backtesting"""
    return render_template('validation.html')

@app.route('/dataset')
def dataset_page():
    """Page de gestion du dataset"""
    return render_template('dataset.html')

@app.route('/static/<path:filename>')
def serve_static(filename):
    """Servir les fichiers statiques"""
    return send_from_directory(app.static_folder, filename)

# ============================================================================
# REST API ENDPOINTS
# ============================================================================

@app.route('/api/status', methods=['GET'])
def get_status():
    """Obtenir le statut du syst√®me"""
    return jsonify({
        'is_training': system_state.is_training,
        'is_trading': system_state.is_trading,
        'current_model': system_state.current_model,
        'equity': system_state.current_equity,
        'position': system_state.current_position,
        'pnl': system_state.current_pnl,
        'daily_trades': len(system_state.daily_trades),
        'training_state': system_state.training_state,
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/config', methods=['GET', 'POST'])
def manage_config():
    """G√©rer la configuration du syst√®me"""
    if request.method == 'GET':
        return jsonify(system_state.config)
    
    elif request.method == 'POST':
        try:
            new_config = request.json
            system_state.config.update(new_config)
            system_state.save_config()
            
            # √âmettre notification de mise √† jour
            socketio.emit('config_updated', new_config)
            
            return jsonify({
                'success': True,
                'message': 'Configuration mise √† jour avec succ√®s'
            })
        except Exception as e:
            logger.error(f"Erreur mise √† jour configuration: {e}")
            return jsonify({
                'success': False,
                'error': str(e)
            }), 500

@app.route('/api/models/list', methods=['GET'])
def list_models():
    """Lister les mod√®les disponibles"""
    try:
        checkpoint_dir = Path(system_state.config['model']['checkpoint_dir'])
        production_dir = Path(system_state.config['model']['production_dir'])
        
        checkpoints = []
        if checkpoint_dir.exists():
            for path in checkpoint_dir.glob('*.pt'):
                stat = path.stat()
                checkpoints.append({
                    'name': path.stem,
                    'path': path.name,  # Envoyer seulement le nom du fichier
                    'size': stat.st_size,
                    'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                    'type': 'checkpoint'
                })

        production = []
        if production_dir.exists():
            for path in production_dir.glob('*.pt'):
                stat = path.stat()
                production.append({
                    'name': path.stem,
                    'path': path.name,  # Envoyer seulement le nom du fichier
                    'size': stat.st_size,
                    'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                    'type': 'production'
                })
        
        return jsonify({
            'checkpoints': checkpoints,
            'production': production
        })
    
    except Exception as e:
        logger.error(f"Erreur listage mod√®les: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/models/load', methods=['POST'])
def load_model():
    """Charger un mod√®le"""
    try:
        data = request.json
        model_filename = data.get('model_path')  # C'est maintenant juste le nom du fichier

        if not model_filename:
            return jsonify({
                'success': False,
                'error': 'Nom de mod√®le manquant'
            }), 400

        # Chercher le fichier dans checkpoint_dir et production_dir
        checkpoint_dir = Path(system_state.config['model']['checkpoint_dir'])
        production_dir = Path(system_state.config['model']['production_dir'])

        model_path = None
        if (checkpoint_dir / model_filename).exists():
            model_path = checkpoint_dir / model_filename
        elif (production_dir / model_filename).exists():
            model_path = production_dir / model_filename

        if not model_path:
            return jsonify({
                'success': False,
                'error': 'Mod√®le non trouv√©'
            }), 404
        
        # Initialiser l'ensemble controller
        logger.info(f"Chargement du mod√®le: {model_path}")
        
        # Charger le data pipeline et feature engineer
        system_state.data_pipeline = DataPipeline()
        
        system_state.feature_engineer = FeatureEngineer()
        
        # Charger l'ensemble
        system_state.ensemble_controller = EnsembleMetaController(
            state_dim=30,
            action_dim=1,
            num_agents=system_state.config['model']['ensemble_size']
        )
        
        # Charger depuis checkpoint
        # PyTorch 2.6+ n√©cessite weights_only=False pour charger des classes personnalis√©es
        checkpoint = torch.load(model_path, weights_only=False)
        system_state.ensemble_controller.load(model_path)
        
        system_state.current_model = Path(model_path).stem
        
        # √âmettre notification
        socketio.emit('model_loaded', {
            'model_name': system_state.current_model,
            'timestamp': datetime.now().isoformat()
        })
        
        logger.info(f"Mod√®le charg√© avec succ√®s: {system_state.current_model}")
        
        return jsonify({
            'success': True,
            'message': f'Mod√®le {system_state.current_model} charg√© avec succ√®s'
        })
    
    except Exception as e:
        logger.error(f"Erreur chargement mod√®le: {e}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/models/delete', methods=['POST'])
def delete_model():
    """Supprimer un mod√®le"""
    try:
        data = request.json
        model_filename = data.get('model_path')  # C'est le nom du fichier

        if not model_filename:
            return jsonify({
                'success': False,
                'error': 'Nom de mod√®le manquant'
            }), 400

        # Chercher le fichier dans checkpoint_dir et production_dir
        checkpoint_dir = Path(system_state.config['model']['checkpoint_dir'])
        production_dir = Path(system_state.config['model']['production_dir'])

        model_path = None
        if (checkpoint_dir / model_filename).exists():
            model_path = checkpoint_dir / model_filename
        elif (production_dir / model_filename).exists():
            model_path = production_dir / model_filename

        if not model_path:
            return jsonify({
                'success': False,
                'error': 'Mod√®le non trouv√©'
            }), 404

        # Supprimer le fichier
        model_path.unlink()
        logger.info(f"Mod√®le supprim√©: {model_path}")

        return jsonify({
            'success': True,
            'message': f'Mod√®le {model_filename} supprim√© avec succ√®s'
        })

    except Exception as e:
        logger.error(f"Erreur suppression mod√®le: {e}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/training/start', methods=['POST'])
def start_training():
    """D√©marrer l'entra√Ænement"""
    if system_state.is_training:
        return jsonify({
            'success': False,
            'error': 'Entra√Ænement d√©j√† en cours'
        }), 400

    try:
        data = request.json

        # Param√®tres d'entra√Ænement
        num_episodes = data.get('num_episodes', 1000)
        batch_size = data.get('batch_size', 1024)  # Augment√© pour mieux utiliser le GPU
        from_checkpoint = data.get('from_checkpoint', None)
        agent_id = data.get('agent_id', None)  # None = tous les agents, sinon l'ID sp√©cifique

        # Initialiser l'√©tat du training
        system_state.training_state = {
            'is_training': True,
            'current_agent': agent_id if agent_id is not None else 'all',
            'current_episode': 0,
            'total_episodes': num_episodes,
            'start_time': datetime.now().isoformat(),
            'metrics': {},
            'training_type': 'sac_agent'
        }
        system_state.save_training_state()

        # Lancer l'entra√Ænement dans un thread s√©par√©
        training_thread = threading.Thread(
            target=run_training,
            args=(num_episodes, batch_size, from_checkpoint, agent_id)
        )
        training_thread.daemon = True
        training_thread.start()

        system_state.training_thread = training_thread
        system_state.is_training = True

        return jsonify({
            'success': True,
            'message': f'Entra√Ænement d√©marr√© (Agent: {agent_id if agent_id is not None else "tous"})'
        })

    except Exception as e:
        logger.error(f"Erreur d√©marrage entra√Ænement: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/training/stop', methods=['POST'])
def stop_training():
    """Arr√™ter l'entra√Ænement"""
    if not system_state.is_training:
        return jsonify({
            'success': False,
            'error': 'Aucun entra√Ænement en cours'
        }), 400

    system_state.stop_event.set()
    system_state.is_training = False

    # Mettre √† jour l'√©tat du training
    system_state.training_state['is_training'] = False
    system_state.save_training_state()

    return jsonify({
        'success': True,
        'message': 'Entra√Ænement arr√™t√©'
    })

@app.route('/api/training/meta-controller/start', methods=['POST'])
def start_meta_controller_training():
    """D√©marrer l'entra√Ænement du meta-controller"""
    if system_state.is_training:
        return jsonify({
            'success': False,
            'error': 'Entra√Ænement d√©j√† en cours'
        }), 400

    try:
        data = request.json

        # Param√®tres d'entra√Ænement
        num_episodes = data.get('num_episodes', 500)
        batch_size = data.get('batch_size', 256)

        # Initialiser l'√©tat du training
        system_state.training_state = {
            'is_training': True,
            'current_agent': 'meta_controller',
            'current_episode': 0,
            'total_episodes': num_episodes,
            'start_time': datetime.now().isoformat(),
            'metrics': {},
            'training_type': 'meta_controller'
        }
        system_state.save_training_state()

        # Lancer l'entra√Ænement dans un thread s√©par√©
        training_thread = threading.Thread(
            target=run_meta_controller_training,
            args=(num_episodes, batch_size)
        )
        training_thread.daemon = True
        training_thread.start()

        system_state.training_thread = training_thread
        system_state.is_training = True

        return jsonify({
            'success': True,
            'message': 'Entra√Ænement du meta-controller d√©marr√©'
        })

    except Exception as e:
        logger.error(f"Erreur d√©marrage entra√Ænement meta-controller: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/trading/start', methods=['POST'])
def start_trading():
    """D√©marrer le trading live"""
    if system_state.is_trading:
        return jsonify({
            'success': False,
            'error': 'Trading d√©j√† actif'
        }), 400
    
    if not system_state.current_model:
        return jsonify({
            'success': False,
            'error': 'Aucun mod√®le charg√©'
        }), 400
    
    try:
        # Initialiser MT5
        from backend.mt5_connector import MT5Config
        mt5_config = MT5Config(
            account=system_state.config.get('mt5', {}).get('login', 0),
            password=system_state.config.get('mt5', {}).get('password', ''),
            server=system_state.config.get('mt5', {}).get('server', '')
        )
        system_state.mt5_connector = MT5Connector(config=mt5_config)
        
        if not system_state.mt5_connector.connect():
            return jsonify({
                'success': False,
                'error': 'Impossible de se connecter √† MT5'
            }), 500
        
        # Initialiser risk manager
        from backend.risk_manager import RiskConfig
        risk_config = RiskConfig(
            risk_per_trade=system_state.config.get('trading', {}).get('risk_per_trade', 0.02),
            daily_loss_limit=system_state.config.get('trading', {}).get('max_daily_loss', 0.05),
            max_drawdown_limit=system_state.config.get('trading', {}).get('max_drawdown', 0.15)
        )
        system_state.risk_manager = RiskManager(
            initial_equity=system_state.config.get('trading', {}).get('initial_capital', 100000.0),
            config=risk_config
        )
        
        # Lancer le trading dans un thread
        trading_thread = threading.Thread(target=run_trading)
        trading_thread.daemon = True
        trading_thread.start()
        
        system_state.trading_thread = trading_thread
        system_state.is_trading = True
        
        return jsonify({
            'success': True,
            'message': 'Trading d√©marr√©'
        })
    
    except Exception as e:
        logger.error(f"Erreur d√©marrage trading: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/dataset/status', methods=['GET'])
def dataset_status():
    """Obtenir le statut du dataset"""
    try:
        data_pipeline = DataPipeline()
        processed_file = data_pipeline.config.processed_data_dir / "processed_data.h5"
        
        status = {
            'processed_exists': processed_file.exists(),
            'processed_path': str(processed_file),
            'raw_data_dir': str(data_pipeline.config.raw_data_dir),
            'processed_data_dir': str(data_pipeline.config.processed_data_dir),
            'pairs': data_pipeline.config.pairs
        }
        
        if processed_file.exists():
            import h5py
            with h5py.File(processed_file, 'r') as f:
                if 'train' in f:
                    train_pairs = list(f['train'].keys())
                    status['train_pairs'] = train_pairs
                    if train_pairs:
                        first_pair = train_pairs[0]
                        status['train_samples'] = len(f[f'train/{first_pair}'])
                if 'validation' in f:
                    val_pairs = list(f['validation'].keys())
                    status['val_pairs'] = val_pairs
                if 'test' in f:
                    test_pairs = list(f['test'].keys())
                    status['test_pairs'] = test_pairs
        
        return jsonify(status)
    
    except Exception as e:
        logger.error(f"Erreur statut dataset: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/dataset/prepare', methods=['POST'])
def prepare_dataset():
    """Pr√©parer le dataset (t√©l√©chargement et traitement)"""
    try:
        data = request.json
        force_download = data.get('force_download', False)
        
        def run_preparation():
            try:
                data_pipeline = DataPipeline()
                train_data, val_data, test_data = data_pipeline.run_full_pipeline(
                    force_download=force_download
                )
                
                socketio.emit('dataset_preparation_complete', {
                    'success': True,
                    'train_samples': len(next(iter(train_data.values()))) if train_data else 0,
                    'val_samples': len(next(iter(val_data.values()))) if val_data else 0,
                    'test_samples': len(next(iter(test_data.values()))) if test_data else 0,
                    'timestamp': datetime.now().isoformat()
                })
            except Exception as e:
                logger.error(f"Erreur pr√©paration dataset: {e}")
                socketio.emit('dataset_preparation_error', {
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                })
        
        # Lancer dans un thread s√©par√©
        prep_thread = threading.Thread(target=run_preparation, daemon=True)
        prep_thread.start()
        
        return jsonify({
            'success': True,
            'message': 'Pr√©paration du dataset d√©marr√©e'
        })
    
    except Exception as e:
        logger.error(f"Erreur d√©marrage pr√©paration: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/trading/stop', methods=['POST'])
def stop_trading():
    """Arr√™ter le trading live"""
    if not system_state.is_trading:
        return jsonify({
            'success': False,
            'error': 'Trading non actif'
        }), 400
    
    system_state.stop_event.set()
    system_state.is_trading = False
    
    # Fermer toutes les positions
    if system_state.mt5_connector:
        system_state.mt5_connector.close_all_positions()
        system_state.mt5_connector.disconnect()
    
    return jsonify({
        'success': True,
        'message': 'Trading arr√™t√©'
    })

@app.route('/api/performance', methods=['GET'])
def get_performance():
    """Obtenir les m√©triques de performance"""
    return jsonify(system_state.performance_metrics)

@app.route('/api/trades', methods=['GET'])
def get_trades():
    """Obtenir l'historique des trades"""
    limit = request.args.get('limit', 100, type=int)
    return jsonify({
        'trades': system_state.trade_history[-limit:]
    })

@app.route('/api/equity-curve', methods=['GET'])
def get_equity_curve():
    """Obtenir la courbe d'equity"""
    return jsonify({
        'equity': system_state.equity_history
    })

@app.route('/api/training/metrics', methods=['GET'])
def get_training_metrics():
    """Obtenir l'historique complet des m√©triques d'entra√Ænement"""
    try:
        metrics_file = Path('logs/training_metrics.json')

        if not metrics_file.exists():
            return jsonify({
                'success': False,
                'error': 'Aucune m√©trique d\'entra√Ænement trouv√©e'
            }), 404

        with open(metrics_file, 'r') as f:
            metrics = json.load(f)

        # Statistiques sur les m√©triques
        num_episodes = len(metrics.get('episodes', []))

        return jsonify({
            'success': True,
            'num_episodes': num_episodes,
            'metrics': metrics,
            'available_metrics': list(metrics.keys()),
            'file_path': str(metrics_file),
            'timestamp': datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration m√©triques: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/validation/run', methods=['POST'])
def run_validation():
    """Ex√©cuter une validation walk-forward"""
    try:
        data = request.json
        model_path = data.get('model_path')
        
        if not model_path:
            return jsonify({
                'success': False,
                'error': 'Chemin du mod√®le requis'
            }), 400
        
        # Cr√©er le validateur
        validator = ValidationFramework(
            data_dir=system_state.config['data']['data_dir']
        )
        
        # Lancer la validation dans un thread
        validation_thread = threading.Thread(
            target=run_validation_process,
            args=(validator, model_path)
        )
        validation_thread.daemon = True
        validation_thread.start()
        
        return jsonify({
            'success': True,
            'message': 'Validation d√©marr√©e'
        })
    
    except Exception as e:
        logger.error(f"Erreur validation: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# ============================================================================
# SOCKETIO HANDLERS
# ============================================================================

@socketio.on('connect')
def handle_connect():
    """Nouveau client connect√©"""
    logger.info(f"Client connect√©: {request.sid}")
    emit('connection_response', {
        'status': 'connected',
        'timestamp': datetime.now().isoformat()
    })

@socketio.on('disconnect')
def handle_disconnect():
    """Client d√©connect√©"""
    logger.info(f"Client d√©connect√©: {request.sid}")

@socketio.on('request_update')
def handle_update_request():
    """Client demande une mise √† jour"""
    emit('status_update', {
        'equity': system_state.current_equity,
        'position': system_state.current_position,
        'pnl': system_state.current_pnl,
        'timestamp': datetime.now().isoformat()
    })

# ============================================================================
# FONCTIONS DE BACKGROUND
# ============================================================================

def run_training(num_episodes: int, batch_size: int, from_checkpoint: Optional[str], agent_id: Optional[int] = None):
    """Ex√©cuter l'entra√Ænement en background"""
    try:
        # Afficher les informations du device
        cuda_available = torch.cuda.is_available()
        device_name = str(torch.cuda.get_device_name(0)) if cuda_available else 'CPU'

        logger.info("="*80)
        logger.info(f"D√©marrage entra√Ænement: {num_episodes} √©pisodes, Agent: {agent_id if agent_id is not None else 'tous'}")
        logger.info(f"Device: {device_name}")
        if cuda_available:
            logger.info(f"GPU Count: {torch.cuda.device_count()}")
            logger.info(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        else:
            logger.warning("‚ö†Ô∏è CUDA non disponible - Entra√Ænement sur CPU (sera plus lent)")
        logger.info("="*80)

        # Notifier le frontend du device utilis√©
        socketio.emit('training_device_info', {
            'device': device_name,
            'cuda_available': cuda_available,
            'timestamp': datetime.now().isoformat()
        })

        # Charger les donn√©es
        data_pipeline = DataPipeline()
        train_data, val_data, test_data = data_pipeline.get_processed_data()

        # Utiliser FeaturePipeline avec cache pour normaliser les features
        from backend.feature_engineering import FeaturePipeline
        feature_pipeline = FeaturePipeline()
        train_features, val_features, test_features = feature_pipeline.run_full_pipeline(
            train_data, val_data, test_data, force_recalculate=False
        )

        logger.info(f"Features charg√©es: {len(train_features)} samples d'entra√Ænement")

        # Retirer la colonne timestamp si pr√©sente (non num√©rique)
        if 'timestamp' in train_features.columns:
            train_features = train_features.drop(columns=['timestamp'])

        logger.info(f"Features apr√®s nettoyage: {train_features.shape}")

        # Utiliser les donn√©es d'entra√Ænement pour l'agent
        eurusd_data = train_data.get('EURUSD')
        if eurusd_data is None:
            raise ValueError("Donn√©es EURUSD non trouv√©es dans le dataset d'entra√Ænement")

        # Cr√©er l'environnement
        from backend.trading_env import TradingEnvConfig
        env_config = TradingEnvConfig(
            initial_capital=system_state.config.get('trading', {}).get('initial_capital', 100000.0)
        )
        env = TradingEnvironment(
            data=eurusd_data,
            features=train_features,
            config=env_config
        )

        # Cr√©er les agents selon agent_id
        agents = []
        if agent_id is not None:
            # Entra√Æner un seul agent
            sac_config = SACConfig(
                state_dim=30,
                action_dim=1,
                hidden_dims=[256, 256]
            )
            agent = SACAgent(config=sac_config, agent_id=agent_id)
            if from_checkpoint:
                # Extraire seulement le nom du fichier (agent.load() ajoute le pr√©fixe du dossier)
                checkpoint_filename = Path(from_checkpoint).name
                agent.load(checkpoint_filename)
            agents.append(agent)
            agent_indices = [agent_id]
        else:
            # Entra√Æner tous les agents
            for i in range(system_state.config['model']['ensemble_size']):
                sac_config = SACConfig(
                    state_dim=30,
                    action_dim=1,
                    hidden_dims=[256, 256]
                )
                # Use agent_id >= 3 to avoid regime feature requirements
                agent = SACAgent(config=sac_config, agent_id=i+3)
                if from_checkpoint:
                    checkpoint_path = from_checkpoint.replace('.pt', f'_agent{i}.pt')
                    # Extraire seulement le nom du fichier (agent.load() ajoute le pr√©fixe du dossier)
                    checkpoint_filename = Path(checkpoint_path).name
                    checkpoint_full_path = Path(system_state.config['model']['checkpoint_dir']) / checkpoint_filename
                    if checkpoint_full_path.exists():
                        agent.load(checkpoint_filename)
                agents.append(agent)
            agent_indices = list(range(len(agents)))

        logger.info(f"Agents cr√©√©s: {len(agents)}")

        # Fonction pour initialiser l'historique des m√©triques
        def initialize_metrics_history():
            """Initialise toutes les m√©triques √† tracker (style TensorBoard)"""
            return {
                # Episode info
                'episodes': [],
                'timestamps': [],

                # Rewards
                'episode_rewards': [],
                'episode_rewards_mean': [],  # Moving average
                'episode_rewards_std': [],

                # Performance metrics
                'sharpe_ratios': [],
                'sortino_ratios': [],
                'win_rates': [],
                'max_drawdowns': [],
                'total_returns': [],
                'final_equities': [],
                'profit_factors': [],
                'total_trades': [],
                'winning_trades': [],
                'losing_trades': [],

                # Losses
                'critic_losses': [],
                'actor_losses': [],
                'alpha_losses': [],

                # SAC specific
                'alpha_values': [],
                'target_entropies': [],
                'policy_entropy': [],
                'entropy_gap': [],

                # Q-values and TD error
                'q1_mean': [],
                'q1_std': [],
                'q_target_mean': [],
                'td_error_mean': [],

                # Learning rates
                'actor_lr': [],
                'critic_lr': [],

                # Buffer stats
                'buffer_sizes': [],
                'buffer_winning_ratio': [],
                'buffer_losing_ratio': [],
                'buffer_neutral_ratio': [],

                # Exploration stats
                'action_mean': [],
                'action_std': [],

                # Step info
                'episode_steps': [],
                'total_steps': [],

                # Gradient stats
                'actor_grad_norm': [],
                'critic_grad_norm': [],
            }

        # Historique des m√©triques pour les graphiques (SANS LIMITE - tout l'historique)
        # Charger l'historique existant s'il existe
        metrics_file = Path('logs/training_metrics.json')
        if metrics_file.exists() and not from_checkpoint:
            try:
                with open(metrics_file, 'r') as f:
                    metrics_history = json.load(f)
                logger.info(f"M√©triques charg√©es: {len(metrics_history.get('episodes', []))} √©pisodes pr√©c√©dents")
            except:
                metrics_history = initialize_metrics_history()
        else:
            metrics_history = initialize_metrics_history()

        # Logger pour les transitions (pour CSV)
        transitions_log = []

        # Entra√Æner chaque agent
        for episode in range(num_episodes):
            # Vider le log de transitions au d√©but de chaque √©pisode pour ne sauvegarder que l'√©pisode du checkpoint
            transitions_log.clear()

            # Logger les transitions seulement pour les √©pisodes qui seront sauvegard√©s (optimisation performance)
            should_log_transitions = (episode % 5 == 0 and episode > 0)

            if system_state.stop_event.is_set():
                logger.info("Arr√™t manuel d√©tect√©, sauvegarde des agents...")
                # Sauvegarder les agents avant d'arr√™ter
                for i, agent in enumerate(agents):
                    current_agent_id = agent_indices[i] if agent_id is None else agent_id
                    filename = f'checkpoint_ep{episode}_agent{current_agent_id}_manual_stop.pt'
                    agent.save(filename)
                    logger.info(f"Checkpoint d'arr√™t manuel sauvegard√©: {filename}")
                break

            for agent_idx, agent in enumerate(agents):
                state = env.reset()
                episode_reward = 0
                episode_steps = 0
                done = False
                # Utiliser moyenne courante au lieu d'accumuler toutes les pertes (√©conomie m√©moire)
                critic_loss_sum = 0.0
                actor_loss_sum = 0.0
                alpha_loss_sum = 0.0
                update_count = 0

                # Nouvelles m√©triques RL
                q1_mean_sum = 0.0
                q1_std_sum = 0.0
                q_target_mean_sum = 0.0
                td_error_mean_sum = 0.0
                policy_entropy_sum = 0.0
                entropy_gap_sum = 0.0
                actor_grad_norm_sum = 0.0
                critic_grad_norm_sum = 0.0

                # Tracking pour actions (exploration)
                episode_actions = []

                # Capturer la date de d√©but d'√©pisode
                episode_start_time = datetime.now()

                while not done:
                    if system_state.stop_event.is_set():
                        break

                    # S√©lectionner action
                    action = agent.select_action(state, deterministic=False)

                    # Tracker l'action pour statistiques
                    action_value = float(action[0]) if hasattr(action, '__len__') else float(action)
                    episode_actions.append(action_value)

                    # Step environment
                    next_state, reward, done, info = env.step(action)

                    # Logger la transition pour le CSV seulement si c'est un √©pisode de checkpoint (optimisation)
                    if should_log_transitions:
                        # Get current index for hidden columns
                        current_idx = env.episode_start + env.current_step - 1  # -1 because step was already incremented

                        # Get raw_close and timestamp from hidden columns
                        raw_close = float(env.raw_close[current_idx]) if hasattr(env, 'raw_close') else float(info.get('equity', 0)) / 100000.0
                        timestamp = env.data.iloc[current_idx]['timestamp'] if current_idx < len(env.data) else datetime.now()

                        transition_data = {
                            'episode': episode + 1,
                            'agent_id': agent_indices[agent_idx] if agent_id is None else agent_id,
                            'step': episode_steps,
                            'action': float(action[0]) if hasattr(action, '__len__') else float(action),
                            'reward': float(reward),
                            'done': int(done),
                            'equity': float(info.get('equity', 0)),
                            'position': float(info.get('position', 0)),
                            'cumulative_reward': episode_reward + reward,
                            'episode_start_time': episode_start_time.isoformat(),
                            # Hidden columns for precise analysis
                            'raw_close': raw_close,  # Non-normalized price
                            'timestamp': timestamp.isoformat() if hasattr(timestamp, 'isoformat') else str(timestamp)  # Exact datetime
                        }

                        # Ajouter les observations (state) - toutes les features
                        if hasattr(state, '__len__'):
                            for i, obs_value in enumerate(state):
                                transition_data[f'obs_{i}'] = float(obs_value)

                        transitions_log.append(transition_data)

                    # Stocker dans replay buffer
                    agent.replay_buffer.push(state, action, reward, next_state, done)

                    # Update agent: ONLY AFTER WARMUP IS COMPLETE
                    # CRITICAL FIX: Don't update during warmup when actions are forced to 0
                    # This prevents learning biased policy from forced actions
                    if len(agent.replay_buffer) > batch_size and env.global_step_count >= env.config.no_trading_warmup_steps:
                        # Log once when updates start (after warmup)
                        if env.global_step_count == env.config.no_trading_warmup_steps:
                            logger.info("="*80)
                            logger.info(f"üöÄ WARMUP COMPLETE - Network updates starting!")
                            logger.info(f"   Buffer filled with {len(agent.replay_buffer)} transitions")
                            logger.info(f"   Agent will now learn from its own actions (not forced actions)")
                            logger.info("="*80)

                        # Faire 2 updates cons√©cutifs (REDUCED from 4)
                        # Reasoning: 4√ó512 = 2048 samples/step was too aggressive
                        # Now: 2√ó256 = 512 samples/step (much better ratio)
                        for _ in range(2):
                            losses = agent.update()
                            if losses:
                                critic_loss_sum += losses.get('critic_loss', 0)
                                actor_loss_sum += losses.get('actor_loss', 0)
                                alpha_loss_sum += losses.get('alpha_loss', 0)

                                # Collecter les nouvelles m√©triques RL
                                q1_mean_sum += losses.get('q1_mean', 0)
                                q1_std_sum += losses.get('q1_std', 0)
                                q_target_mean_sum += losses.get('q_target_mean', 0)
                                td_error_mean_sum += losses.get('td_error_mean', 0)
                                policy_entropy_sum += losses.get('policy_entropy', 0)
                                entropy_gap_sum += losses.get('entropy_gap', 0)
                                actor_grad_norm_sum += losses.get('actor_grad_norm', 0)
                                critic_grad_norm_sum += losses.get('critic_grad_norm', 0)

                                update_count += 1

                    episode_reward += reward
                    episode_steps += 1
                    agent.total_steps += 1  # CRITICAL: Increment total steps for LR scheduling
                    state = next_state

                    # √âmettre la progression de l'√©pisode tous les 20 steps (optimisation performance)
                    if episode_steps % 20 == 0:
                        socketio.emit('episode_step_progress', {
                            'current_step': int(episode_steps),
                            'episode_length': int(env.episode_length),
                            'episode': int(episode + 1)
                        })

                # Capturer la date de fin d'√©pisode et l'ajouter √† la derni√®re transition
                if should_log_transitions:
                    episode_end_time = datetime.now()
                    if len(transitions_log) > 0:
                        transitions_log[-1]['episode_end_time'] = episode_end_time.isoformat()

                # Calculer m√©triques de l'√©pisode
                env_metrics = env.get_episode_metrics()

                # Calculer statistiques des actions
                action_mean = float(np.mean(episode_actions)) if episode_actions else 0.0
                action_std = float(np.std(episode_actions)) if episode_actions else 0.0

                # Calculer moving average des rewards (100 derniers √©pisodes)
                recent_rewards = metrics_history['episode_rewards'][-99:] + [episode_reward]
                reward_mean = float(np.mean(recent_rewards))
                reward_std = float(np.std(recent_rewards))

                # R√©cup√©rer les stats du buffer
                buffer_size = len(agent.replay_buffer)
                buffer_winning = len(agent.replay_buffer.winning_indices) / buffer_size if buffer_size > 0 else 0
                buffer_losing = len(agent.replay_buffer.losing_indices) / buffer_size if buffer_size > 0 else 0
                buffer_neutral = len(agent.replay_buffer.neutral_indices) / buffer_size if buffer_size > 0 else 0

                # R√©cup√©rer les learning rates actuels
                actor_lr = float(agent.actor_optimizer.param_groups[0]['lr'])
                critic_lr = float(agent.critic_optimizer.param_groups[0]['lr'])

                # Mettre √† jour l'√©tat du training
                current_agent_id = agent_indices[agent_idx] if agent_id is None else agent_id
                system_state.training_state['current_episode'] = episode + 1
                system_state.training_state['metrics'] = {
                    'agent_id': current_agent_id,
                    'episode_reward': float(episode_reward),
                    'episode_steps': episode_steps,
                    'avg_critic_loss': float(critic_loss_sum / update_count) if update_count > 0 else 0,
                    'avg_actor_loss': float(actor_loss_sum / update_count) if update_count > 0 else 0,
                    'avg_alpha_loss': float(alpha_loss_sum / update_count) if update_count > 0 else 0,
                    'sharpe_ratio': float(env_metrics.get('sharpe_ratio', 0)),
                    'sortino_ratio': float(env_metrics.get('sortino_ratio', 0)),
                    'max_drawdown': float(env_metrics.get('max_drawdown', 0)),
                    'win_rate': float(env_metrics.get('win_rate', 0)),
                    'profit_factor': float(env_metrics.get('profit_factor', 0)),
                    'total_return': float(env_metrics.get('total_return', 0))
                }

                # Ajouter TOUTES les m√©triques √† l'historique (SANS LIMITE - stockage complet)
                metrics_history['episodes'].append(int(episode + 1))
                metrics_history['timestamps'].append(datetime.now().isoformat())

                # Rewards
                metrics_history['episode_rewards'].append(float(episode_reward))
                metrics_history['episode_rewards_mean'].append(reward_mean)
                metrics_history['episode_rewards_std'].append(reward_std)

                # Performance metrics
                metrics_history['sharpe_ratios'].append(float(env_metrics.get('sharpe_ratio', 0)))
                metrics_history['sortino_ratios'].append(float(env_metrics.get('sortino_ratio', 0)))
                metrics_history['win_rates'].append(float(env_metrics.get('win_rate', 0)))
                metrics_history['max_drawdowns'].append(float(env_metrics.get('max_drawdown', 0)))
                metrics_history['total_returns'].append(float(env_metrics.get('total_return', 0)))
                metrics_history['final_equities'].append(float(env_metrics.get('final_equity', 0)))
                metrics_history['profit_factors'].append(float(env_metrics.get('profit_factor', 0)))
                metrics_history['total_trades'].append(int(env_metrics.get('total_trades', 0)))
                metrics_history['winning_trades'].append(int(env_metrics.get('winning_trades', 0)))
                metrics_history['losing_trades'].append(int(env_metrics.get('losing_trades', 0)))

                # Losses
                metrics_history['critic_losses'].append(float(critic_loss_sum / update_count) if update_count > 0 else 0)
                metrics_history['actor_losses'].append(float(actor_loss_sum / update_count) if update_count > 0 else 0)
                metrics_history['alpha_losses'].append(float(alpha_loss_sum / update_count) if update_count > 0 else 0)

                # SAC specific
                metrics_history['alpha_values'].append(float(agent.alpha.item()))
                metrics_history['target_entropies'].append(float(agent.target_entropy) if hasattr(agent, 'target_entropy') else 0)
                metrics_history['policy_entropy'].append(float(policy_entropy_sum / update_count) if update_count > 0 else 0)
                metrics_history['entropy_gap'].append(float(entropy_gap_sum / update_count) if update_count > 0 else 0)

                # Q-values and TD error
                metrics_history['q1_mean'].append(float(q1_mean_sum / update_count) if update_count > 0 else 0)
                metrics_history['q1_std'].append(float(q1_std_sum / update_count) if update_count > 0 else 0)
                metrics_history['q_target_mean'].append(float(q_target_mean_sum / update_count) if update_count > 0 else 0)
                metrics_history['td_error_mean'].append(float(td_error_mean_sum / update_count) if update_count > 0 else 0)

                # Learning rates
                metrics_history['actor_lr'].append(actor_lr)
                metrics_history['critic_lr'].append(critic_lr)

                # Buffer stats
                metrics_history['buffer_sizes'].append(buffer_size)
                metrics_history['buffer_winning_ratio'].append(float(buffer_winning))
                metrics_history['buffer_losing_ratio'].append(float(buffer_losing))
                metrics_history['buffer_neutral_ratio'].append(float(buffer_neutral))

                # Exploration stats
                metrics_history['action_mean'].append(action_mean)
                metrics_history['action_std'].append(action_std)

                # Step info
                metrics_history['episode_steps'].append(episode_steps)
                metrics_history['total_steps'].append(int(agent.total_steps))

                # Gradient stats
                metrics_history['actor_grad_norm'].append(float(actor_grad_norm_sum / update_count) if update_count > 0 else 0)
                metrics_history['critic_grad_norm'].append(float(critic_grad_norm_sum / update_count) if update_count > 0 else 0)

                # PAS DE LIMITATION - On garde tout l'historique depuis l'√©pisode 0

                # Sauvegarder l'√©tat ET les m√©triques tous les √©pisodes
                system_state.save_training_state()

                # Sauvegarder l'historique complet des m√©triques dans un fichier JSON
                try:
                    with open(metrics_file, 'w') as f:
                        json.dump(metrics_history, f, indent=2)
                    logger.info(f"M√©triques sauvegard√©es: {len(metrics_history['episodes'])} √©pisodes")
                except Exception as e:
                    logger.error(f"Erreur sauvegarde m√©triques: {e}")

                # Envoyer TOUT l'historique au frontend (pas de limitation)
                # Le frontend peut g√©rer l'affichage (zoom, pan, etc.)
                history_to_send = metrics_history

                # √âmettre progression √† chaque √©pisode avec TOUTES les m√©triques
                socketio.emit('training_progress', {
                    # Episode info
                    'episode': int(episode + 1),
                    'total_episodes': int(num_episodes),
                    'agent': int(current_agent_id),
                    'steps': int(episode_steps),
                    'total_steps': int(agent.total_steps),
                    'episode_length': int(env.episode_length),

                    # Current episode metrics
                    'reward': float(episode_reward),
                    'reward_mean': reward_mean,
                    'reward_std': reward_std,

                    # Losses
                    'critic_loss': float(critic_loss_sum / update_count) if update_count > 0 else 0.0,
                    'actor_loss': float(actor_loss_sum / update_count) if update_count > 0 else 0.0,
                    'alpha_loss': float(alpha_loss_sum / update_count) if update_count > 0 else 0.0,

                    # SAC specific
                    'alpha': float(agent.alpha.item()),
                    'target_entropy': float(agent.target_entropy) if hasattr(agent, 'target_entropy') else 0,

                    # Learning rates
                    'actor_lr': actor_lr,
                    'critic_lr': critic_lr,

                    # Buffer stats
                    'buffer_size': buffer_size,
                    'buffer_winning_ratio': float(buffer_winning),
                    'buffer_losing_ratio': float(buffer_losing),
                    'buffer_neutral_ratio': float(buffer_neutral),

                    # Exploration
                    'action_mean': action_mean,
                    'action_std': action_std,

                    # Performance metrics
                    'sharpe_ratio': float(env_metrics.get('sharpe_ratio', 0)),
                    'sortino_ratio': float(env_metrics.get('sortino_ratio', 0)),
                    'max_drawdown': float(env_metrics.get('max_drawdown', 0)),
                    'win_rate': float(env_metrics.get('win_rate', 0)),
                    'profit_factor': float(env_metrics.get('profit_factor', 0)),
                    'total_return': float(env_metrics.get('total_return', 0)),
                    'total_trades': int(env_metrics.get('total_trades', 0)),
                    'winning_trades': int(env_metrics.get('winning_trades', 0)),
                    'losing_trades': int(env_metrics.get('losing_trades', 0)),
                    'final_equity': float(env_metrics.get('final_equity', 0)),

                    # Timestamp
                    'timestamp': datetime.now().isoformat(),

                    # TOUT l'historique des m√©triques (depuis l'√©pisode 0)
                    'metrics_history': history_to_send
                })

            # Sauvegarder checkpoint et CSV tous les 5 √©pisodes
            if episode % 5 == 0 and episode > 0:
                for i, agent in enumerate(agents):
                    current_agent_id = agent_indices[i] if agent_id is None else agent_id
                    filename = f'checkpoint_ep{episode}_agent{current_agent_id}.pt'
                    agent.save(filename)
                    logger.info(f"Checkpoint sauvegard√©: {filename}")

                # Sauvegarder les transitions dans un CSV (seulement l'√©pisode du checkpoint)
                if len(transitions_log) > 0:
                    import pandas as pd
                    csv_dir = Path('logs/training_csvs')
                    csv_dir.mkdir(parents=True, exist_ok=True)

                    df = pd.DataFrame(transitions_log)
                    # Nom du fichier indique l'√©pisode du checkpoint
                    csv_filename = csv_dir / f'training_ep{episode}_agent{current_agent_id}.csv'
                    df.to_csv(csv_filename, index=False)
                    logger.info(f"Training CSV sauvegard√©: {csv_filename} ({len(transitions_log)} transitions)")
        
        logger.info("Entra√Ænement termin√©")
        system_state.is_training = False
        system_state.stop_event.clear()

        # Mettre √† jour l'√©tat final
        system_state.training_state['is_training'] = False
        system_state.save_training_state()

        # Sauvegarde FINALE de toutes les m√©triques
        try:
            with open(metrics_file, 'w') as f:
                json.dump(metrics_history, f, indent=2)
            logger.info(f"‚úÖ M√©triques finales sauvegard√©es: {len(metrics_history['episodes'])} √©pisodes au total")
            logger.info(f"   Fichier: {metrics_file}")
        except Exception as e:
            logger.error(f"Erreur sauvegarde m√©triques finales: {e}")

        socketio.emit('training_complete', {
            'message': 'Entra√Ænement termin√© avec succ√®s',
            'agent_id': agent_id if agent_id is not None else 'all',
            'total_episodes': len(metrics_history['episodes']),
            'metrics_file': str(metrics_file),
            'timestamp': datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"Erreur pendant l'entra√Ænement: {e}")
        logger.error(traceback.format_exc())
        system_state.is_training = False
        system_state.stop_event.clear()

        # Mettre √† jour l'√©tat en cas d'erreur
        system_state.training_state['is_training'] = False
        system_state.save_training_state()

        socketio.emit('training_error', {
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        })

def run_meta_controller_training(num_episodes: int, batch_size: int):
    """Ex√©cuter l'entra√Ænement du meta-controller en background"""
    try:
        # Afficher les informations du device
        cuda_available = torch.cuda.is_available()
        device_name = str(torch.cuda.get_device_name(0)) if cuda_available else 'CPU'

        logger.info("="*80)
        logger.info(f"D√©marrage entra√Ænement meta-controller: {num_episodes} √©pisodes")
        logger.info(f"Device: {device_name}")
        if cuda_available:
            logger.info(f"GPU Count: {torch.cuda.device_count()}")
            logger.info(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        else:
            logger.warning("‚ö†Ô∏è CUDA non disponible - Entra√Ænement sur CPU (sera plus lent)")
        logger.info("="*80)

        # Notifier le frontend du device utilis√©
        socketio.emit('training_device_info', {
            'device': device_name,
            'cuda_available': cuda_available,
            'timestamp': datetime.now().isoformat()
        })

        # Charger les donn√©es
        data_pipeline = DataPipeline()
        train_data, val_data, test_data = data_pipeline.get_processed_data()

        # Utiliser FeaturePipeline avec cache
        from backend.feature_engineering import FeaturePipeline
        feature_pipeline = FeaturePipeline()
        train_features, val_features, test_features = feature_pipeline.run_full_pipeline(
            train_data, val_data, test_data, force_recalculate=False
        )

        # Retirer la colonne timestamp si pr√©sente (non num√©rique)
        if 'timestamp' in train_features.columns:
            train_features = train_features.drop(columns=['timestamp'])

        # Charger les 3 agents SAC pr√©-entra√Æn√©s
        agents = []
        checkpoint_dir = Path(system_state.config['model']['checkpoint_dir'])

        for i in range(3):
            sac_config = SACConfig(
                state_dim=30,
                action_dim=1,
                hidden_dims=[256, 256]
            )
            agent = SACAgent(config=sac_config, agent_id=i+3)

            # Trouver le dernier checkpoint pour cet agent
            checkpoints = list(checkpoint_dir.glob(f'checkpoint_*_agent{i+3}.pt'))
            if checkpoints:
                latest_checkpoint = max(checkpoints, key=lambda p: p.stat().st_mtime)
                agent.load(latest_checkpoint.name)  # Passer seulement le nom du fichier
                logger.info(f"Agent {i+3} charg√© depuis: {latest_checkpoint.name}")
            else:
                logger.warning(f"Aucun checkpoint trouv√© pour l'agent {i+3}")

            agents.append(agent)

        # Cr√©er le meta-controller
        meta_controller = EnsembleMetaController(
            state_dim=30,
            action_dim=1,
            num_agents=3
        )

        # Cr√©er l'environnement
        eurusd_data = train_data.get('EURUSD')
        from backend.trading_env import TradingEnvConfig
        env_config = TradingEnvConfig(
            initial_capital=system_state.config.get('trading', {}).get('initial_capital', 100000.0)
        )
        env = TradingEnvironment(
            data=eurusd_data,
            features=train_features,
            config=env_config
        )

        # Logger pour les transitions (pour CSV)
        transitions_log = []

        # Entra√Æner le meta-controller
        for episode in range(num_episodes):
            # Vider le log de transitions au d√©but de chaque √©pisode pour ne sauvegarder que l'√©pisode du checkpoint
            transitions_log.clear()

            # Logger les transitions seulement pour les √©pisodes qui seront sauvegard√©s (optimisation performance)
            should_log_transitions = (episode % 50 == 0 and episode > 0)

            if system_state.stop_event.is_set():
                break

            state = env.reset()
            episode_reward = 0
            episode_steps = 0
            done = False

            # Capturer la date de d√©but d'√©pisode
            episode_start_time = datetime.now()

            while not done:
                if system_state.stop_event.is_set():
                    break

                # Obtenir les actions de tous les agents
                agent_actions = []
                for agent in agents:
                    action = agent.select_action(state, deterministic=True)
                    agent_actions.append(action)

                # Le meta-controller apprend √† pond√©rer les actions
                final_action = meta_controller.aggregate_actions(state, agent_actions)

                # Step environment
                next_state, reward, done, info = env.step(final_action)

                # Logger la transition pour le CSV seulement si c'est un √©pisode de checkpoint (optimisation)
                if should_log_transitions:
                    transition_data = {
                        'episode': episode + 1,
                        'agent_id': 'meta_controller',
                        'step': episode_steps,
                        'final_action': float(final_action[0]) if hasattr(final_action, '__len__') else float(final_action),
                        'reward': float(reward),
                        'done': int(done),
                        'equity': float(info.get('equity', 0)),
                        'position': float(info.get('position', 0)),
                        'cumulative_reward': episode_reward + reward,
                        'episode_start_time': episode_start_time.isoformat()
                    }

                    # Ajouter les actions des agents individuels
                    for i, agent_action in enumerate(agent_actions):
                        action_val = float(agent_action[0]) if hasattr(agent_action, '__len__') else float(agent_action)
                        transition_data[f'agent_{i+3}_action'] = action_val

                    # Ajouter les observations (state)
                    if hasattr(state, '__len__'):
                        for i, obs_value in enumerate(state):
                            transition_data[f'obs_{i}'] = float(obs_value)

                    transitions_log.append(transition_data)

                # Entra√Æner le meta-controller
                meta_controller.train_step(state, agent_actions, reward, next_state, done)

                episode_reward += reward
                episode_steps += 1
                state = next_state

                # √âmettre la progression de l'√©pisode tous les 20 steps (optimisation performance)
                if episode_steps % 20 == 0:
                    socketio.emit('episode_step_progress', {
                        'current_step': int(episode_steps),
                        'episode_length': int(env.episode_length),
                        'episode': int(episode + 1)
                    })

            # Capturer la date de fin d'√©pisode et l'ajouter √† la derni√®re transition
            if should_log_transitions:
                episode_end_time = datetime.now()
                if len(transitions_log) > 0:
                    transitions_log[-1]['episode_end_time'] = episode_end_time.isoformat()

            # Calculer m√©triques de l'√©pisode
            env_metrics = env.get_episode_metrics()

            # Mettre √† jour l'√©tat du training
            system_state.training_state['current_episode'] = episode + 1
            system_state.training_state['metrics'] = {
                'agent_id': 'meta_controller',
                'episode_reward': float(episode_reward),
                'episode_steps': episode_steps,
                'sharpe_ratio': float(env_metrics.get('sharpe_ratio', 0)),
                'sortino_ratio': float(env_metrics.get('sortino_ratio', 0)),
                'max_drawdown': float(env_metrics.get('max_drawdown', 0)),
                'win_rate': float(env_metrics.get('win_rate', 0)),
                'profit_factor': float(env_metrics.get('profit_factor', 0)),
                'total_return': float(env_metrics.get('total_return', 0))
            }

            # √âmettre progression √† chaque √©pisode
            system_state.save_training_state()
            socketio.emit('training_progress', {
                'episode': int(episode + 1),
                'total_episodes': int(num_episodes),
                'agent': 'meta_controller',
                'reward': float(episode_reward),
                'steps': int(episode_steps),
                'episode_length': int(env.episode_length),  # Pour la barre de progression d'√©pisode
                'sharpe_ratio': float(env_metrics.get('sharpe_ratio', 0)),
                'sortino_ratio': float(env_metrics.get('sortino_ratio', 0)),
                'max_drawdown': float(env_metrics.get('max_drawdown', 0)),
                'win_rate': float(env_metrics.get('win_rate', 0)),
                'profit_factor': float(env_metrics.get('profit_factor', 0)),
                'total_return': float(env_metrics.get('total_return', 0)),
                'timestamp': datetime.now().isoformat()
            })

            # Sauvegarder checkpoint tous les 50 √©pisodes
            if episode % 50 == 0 and episode > 0:
                filename = f'meta_controller_ep{episode}.pt'
                meta_controller.save(filename)
                logger.info(f"Meta-controller checkpoint sauvegard√©: {filename}")

                # Sauvegarder les transitions dans un CSV
                if len(transitions_log) > 0:
                    import pandas as pd
                    csv_dir = Path('logs/training_csvs')
                    csv_dir.mkdir(parents=True, exist_ok=True)

                    df = pd.DataFrame(transitions_log)
                    csv_filename = csv_dir / f'training_meta_controller_ep{episode}.csv'
                    df.to_csv(csv_filename, index=False)
                    logger.info(f"Training CSV sauvegard√©: {csv_filename} ({len(transitions_log)} transitions)")

        logger.info("Entra√Ænement meta-controller termin√©")
        system_state.is_training = False
        system_state.stop_event.clear()

        # Mettre √† jour l'√©tat final
        system_state.training_state['is_training'] = False
        system_state.save_training_state()

        socketio.emit('training_complete', {
            'message': 'Entra√Ænement meta-controller termin√© avec succ√®s',
            'agent_id': 'meta_controller',
            'timestamp': datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"Erreur pendant l'entra√Ænement meta-controller: {e}")
        logger.error(traceback.format_exc())
        system_state.is_training = False
        system_state.stop_event.clear()

        # Mettre √† jour l'√©tat en cas d'erreur
        system_state.training_state['is_training'] = False
        system_state.save_training_state()

        socketio.emit('training_error', {
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        })

def run_trading():
    """Ex√©cuter le trading live en background"""
    try:
        logger.info("D√©marrage trading live")
        
        # Boucle de trading
        while not system_state.stop_event.is_set():
            try:
                # V√©rifier que le march√© est ouvert
                if not system_state.mt5_connector.is_market_open():
                    logger.info("March√© ferm√©, pause...")
                    time.sleep(60)
                    continue
                
                # Obtenir les derni√®res donn√©es
                current_data = system_state.mt5_connector.get_current_data()
                
                # Calculer les features
                features = system_state.feature_engineer.calculate_features(current_data)
                current_state = features[-1]  # Derni√®re observation
                
                # Obtenir l'action de l'ensemble
                action = system_state.ensemble_controller.predict(current_state)
                
                # V√©rifier avec le risk manager
                can_trade, reason = system_state.risk_manager.can_trade(
                    current_equity=system_state.current_equity,
                    current_position=system_state.current_position
                )
                
                if not can_trade:
                    logger.warning(f"Trade refus√©: {reason}")
                    time.sleep(60)
                    continue
                
                # Ex√©cuter l'action si significative
                if abs(action) > 0.1:  # Seuil pour √©viter micro-trades
                    # Calculer position size
                    position_size = system_state.risk_manager.calculate_position_size(
                        action=action,
                        equity=system_state.current_equity,
                        atr=current_state[6]  # ATR feature
                    )
                    
                    # Passer l'ordre
                    if action > 0:  # Long
                        result = system_state.mt5_connector.open_position(
                            symbol=system_state.config['mt5']['symbol'],
                            order_type='buy',
                            volume=position_size
                        )
                    else:  # Short
                        result = system_state.mt5_connector.open_position(
                            symbol=system_state.config['mt5']['symbol'],
                            order_type='sell',
                            volume=abs(position_size)
                        )
                    
                    if result['success']:
                        logger.info(f"Position ouverte: {result}")
                        
                        # Enregistrer le trade
                        trade = {
                            'timestamp': datetime.now().isoformat(),
                            'action': 'long' if action > 0 else 'short',
                            'size': position_size,
                            'price': result.get('price'),
                            'sl': result.get('sl'),
                            'tp': result.get('tp')
                        }
                        system_state.trade_history.append(trade)
                        system_state.daily_trades.append(trade)
                        
                        # √âmettre via SocketIO
                        socketio.emit('new_trade', trade)
                
                # Mettre √† jour les m√©triques
                account_info = system_state.mt5_connector.get_account_info()
                system_state.current_equity = account_info['equity']
                system_state.current_position = system_state.mt5_connector.get_current_position()
                system_state.current_pnl = account_info['profit']
                
                system_state.equity_history.append({
                    'timestamp': datetime.now().isoformat(),
                    'equity': system_state.current_equity
                })
                
                # √âmettre mise √† jour
                socketio.emit('status_update', {
                    'equity': system_state.current_equity,
                    'position': system_state.current_position,
                    'pnl': system_state.current_pnl,
                    'timestamp': datetime.now().isoformat()
                })
                
                # Attendre prochaine barre (5 minutes)
                time.sleep(300)
            
            except Exception as e:
                logger.error(f"Erreur dans la boucle de trading: {e}")
                logger.error(traceback.format_exc())
                time.sleep(60)
        
        logger.info("Trading arr√™t√©")
        system_state.is_trading = False
        system_state.stop_event.clear()
    
    except Exception as e:
        logger.error(f"Erreur fatale trading: {e}")
        logger.error(traceback.format_exc())
        system_state.is_trading = False
        system_state.stop_event.clear()

def run_validation_process(validator: ValidationFramework, model_path: str):
    """Ex√©cuter le processus de validation"""
    try:
        logger.info(f"D√©marrage validation pour: {model_path}")

        # Charger le mod√®le
        sac_config = SACConfig(
            state_dim=30,
            action_dim=1,
            hidden_dims=[256, 256]
        )
        agent = SACAgent(config=sac_config, agent_id=1)
        agent.load(model_path)

        # Ex√©cuter validation walk-forward
        results = validator.walk_forward_validation(
            agent=agent,
            n_folds=5,
            train_size=0.7
        )
        
        # Calculer m√©triques statistiques
        stats = validator.compute_statistical_tests(results)
        
        # Sauvegarder r√©sultats
        report_path = Path('reports') / f'validation_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_path, 'w') as f:
            json.dump({
                'results': results,
                'statistics': stats,
                'model_path': model_path,
                'timestamp': datetime.now().isoformat()
            }, f, indent=2)
        
        # √âmettre r√©sultats
        socketio.emit('validation_complete', {
            'results': results,
            'statistics': stats,
            'report_path': str(report_path),
            'timestamp': datetime.now().isoformat()
        })
        
        logger.info(f"Validation termin√©e: {report_path}")
    
    except Exception as e:
        logger.error(f"Erreur validation: {e}")
        logger.error(traceback.format_exc())
        
        socketio.emit('validation_error', {
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        })

def monitoring_loop():
    """Boucle de monitoring en arri√®re-plan"""
    while True:
        try:
            if system_state.is_trading:
                # V√©rifier √©tat du syst√®me
                account_info = system_state.mt5_connector.get_account_info()
                
                # Calculer drawdown
                peak_equity = max([e['equity'] for e in system_state.equity_history] + [system_state.current_equity])
                current_dd = (peak_equity - system_state.current_equity) / peak_equity
                
                # V√©rifier limites de risque
                if current_dd > system_state.config['trading']['max_drawdown']:
                    logger.warning(f"Drawdown maximum atteint: {current_dd:.2%}")
                    socketio.emit('risk_alert', {
                        'type': 'max_drawdown',
                        'value': current_dd,
                        'timestamp': datetime.now().isoformat()
                    })
                    
                    # Arr√™ter le trading
                    stop_trading()
            
            time.sleep(60)  # Check toutes les minutes
        
        except Exception as e:
            logger.error(f"Erreur monitoring: {e}")
            time.sleep(60)

# ============================================================================
# MAIN
# ============================================================================

if __name__ == '__main__':
    # Cr√©er les dossiers n√©cessaires
    for folder in ['logs', 'models/checkpoints', 'models/production', 'config', 'reports', 'static', 'templates']:
        Path(folder).mkdir(parents=True, exist_ok=True)
    
    # D√©marrer le thread de monitoring
    monitoring_thread = threading.Thread(target=monitoring_loop, daemon=True)
    monitoring_thread.start()
    
    # Lancer l'application
    logger.info("D√©marrage de l'application web...")
    logger.info("Interface accessible sur http://localhost:5000")
    
    socketio.run(
        app,
        host='0.0.0.0',
        port=5000,
        debug=False,
        allow_unsafe_werkzeug=True
    )
