"""
Script d'entra√Ænement SAC pour Kaggle
======================================

Ce script permet d'entra√Æner les agents SAC sur Kaggle avec des donn√©es h5 pr√©-process√©es.
Il est 100% compatible avec le code local et produit des mod√®les directement utilisables.

Usage sur Kaggle:
    1. Upload le fichier processed_data.h5 dans /kaggle/input/
    2. Copier tout le dossier backend/ dans le notebook
    3. Ex√©cuter ce script

Auteur: Trading SAC System
Date: 2025-11-23
"""

import os
import sys
import time
import json
import argparse
from pathlib import Path
from typing import Dict, Any
import numpy as np
import torch

# Ajouter le chemin backend pour les imports
sys.path.insert(0, str(Path(__file__).parent))

from backend.data_pipeline import DataPipeline
from backend.feature_engineering import FeaturePipeline
from backend.trading_env import TradingEnvironment, TradingEnvConfig
from backend.sac_agent import SACAgent, SACConfig


class KaggleTrainer:
    """Trainer optimis√© pour environnement Kaggle"""

    def __init__(
        self,
        h5_path: str = "/kaggle/input/trading-data/processed_data.h5",
        output_dir: str = "/kaggle/working",
        num_episodes: int = 100,
        eval_frequency: int = 10,
        checkpoint_frequency: int = 5,
        agent_id: int = 1,
        device: str = "auto"
    ):
        """
        Args:
            h5_path: Chemin vers le fichier h5 avec les donn√©es
            output_dir: R√©pertoire de sortie pour les mod√®les
            num_episodes: Nombre d'√©pisodes d'entra√Ænement
            eval_frequency: Fr√©quence d'√©valuation (en √©pisodes)
            checkpoint_frequency: Fr√©quence de sauvegarde (en √©pisodes)
            agent_id: ID de l'agent (1, 2, ou 3)
            device: Device PyTorch ('cuda', 'cpu', ou 'auto')
        """
        self.h5_path = h5_path
        self.output_dir = Path(output_dir)
        self.num_episodes = num_episodes
        self.eval_frequency = eval_frequency
        self.checkpoint_frequency = checkpoint_frequency
        self.agent_id = agent_id

        # Configuration du device
        if device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device

        print(f"üöÄ Initialisation KaggleTrainer")
        print(f"   Device: {self.device}")
        print(f"   Agent ID: {self.agent_id}")
        print(f"   Episodes: {self.num_episodes}")
        print(f"   Output: {self.output_dir}")

        # Cr√©er les r√©pertoires de sortie
        self.checkpoints_dir = self.output_dir / "checkpoints"
        self.logs_dir = self.output_dir / "logs"
        self.checkpoints_dir.mkdir(parents=True, exist_ok=True)
        self.logs_dir.mkdir(parents=True, exist_ok=True)

        # M√©triques d'entra√Ænement
        self.training_stats = {
            'episode_rewards': [],
            'episode_lengths': [],
            'eval_rewards': [],
            'actor_losses': [],
            'critic_losses': [],
            'alpha_values': [],
        }

    def load_data(self):
        """Charge les donn√©es depuis le fichier h5"""
        print("\nüìä Chargement des donn√©es...")

        # V√©rifier que le fichier existe
        if not os.path.exists(self.h5_path):
            raise FileNotFoundError(
                f"Fichier h5 introuvable: {self.h5_path}\n"
                f"Assurez-vous d'avoir upload√© processed_data.h5 dans Kaggle Input."
            )

        # Charger via DataPipeline (qui g√®re le cache h5)
        data_pipeline = DataPipeline(processed_data_path=self.h5_path)
        train_data, val_data, test_data = data_pipeline.get_processed_data()

        print(f"‚úÖ Donn√©es charg√©es:")
        print(f"   Train: {len(train_data['EURUSD'])} candles")
        print(f"   Val:   {len(val_data['EURUSD'])} candles")
        print(f"   Test:  {len(test_data['EURUSD'])} candles")

        return train_data, val_data, test_data

    def compute_features(self, train_data, val_data, test_data):
        """Calcule les features avec FeaturePipeline"""
        print("\nüîß Calcul des features...")

        feature_pipeline = FeaturePipeline()
        train_features, val_features, test_features = feature_pipeline.run_full_pipeline(
            train_data, val_data, test_data
        )

        print(f"‚úÖ Features calcul√©es: {train_features.shape[1]} dimensions")
        print(f"   Train shape: {train_features.shape}")
        print(f"   Val shape:   {val_features.shape}")
        print(f"   Test shape:  {test_features.shape}")

        return train_features, val_features, test_features

    def create_environments(self, train_data, val_data, train_features, val_features):
        """Cr√©e les environnements d'entra√Ænement et de validation"""
        print("\nüåç Cr√©ation des environnements...")

        # Configuration de l'environnement (m√™me config que le syst√®me local)
        env_config = TradingEnvConfig(
            initial_capital=500000.0,
            risk_per_trade=0.0005,
            max_leverage=2.0,
            sl_atr_multiplier=3.0,
            tp_atr_multiplier=6.0,
            episode_lengths=[3000],  # 3000 steps = ~5 jours
            no_trading_warmup_steps=5000,
            use_simple_reward=True,
        )

        # Environnement d'entra√Ænement
        train_env = TradingEnvironment(
            data=train_data['EURUSD'],
            features=train_features,
            config=env_config,
            eval_mode=False  # Mode al√©atoire pour exploration
        )

        # Environnement de validation
        val_env = TradingEnvironment(
            data=val_data['EURUSD'],
            features=val_features,
            config=env_config,
            eval_mode=True  # Mode s√©quentiel pour √©valuation
        )

        print(f"‚úÖ Environnements cr√©√©s:")
        print(f"   Action space: {train_env.action_space}")
        print(f"   Observation space: {train_env.observation_space}")

        return train_env, val_env, env_config

    def create_agent(self):
        """Cr√©e l'agent SAC avec la configuration appropri√©e"""
        print("\nü§ñ Cr√©ation de l'agent SAC...")

        # Configuration SAC (identique au syst√®me local)
        sac_config = SACConfig(
            state_dim=30,  # 30 features
            action_dim=1,  # 1 action (position sizing)
            hidden_dims=[256, 256],
            actor_lr=3e-4,
            critic_lr=3e-4,
            alpha_lr=3e-5,
            gamma=0.95,
            tau=0.005,
            warmup_steps=5000,
            buffer_capacity=100000,
            batch_size=1024,
            auto_entropy_tuning=True,
            target_entropy=-1.0,
        )

        # Cr√©er l'agent
        agent = SACAgent(
            config=sac_config,
            agent_id=self.agent_id,
            device=self.device
        )

        print(f"‚úÖ Agent cr√©√©:")
        print(f"   State dim: {sac_config.state_dim}")
        print(f"   Hidden dims: {sac_config.hidden_dims}")
        print(f"   Warmup steps: {sac_config.warmup_steps}")
        print(f"   Buffer size: {sac_config.buffer_capacity}")

        return agent, sac_config

    def train_episode(self, agent: SACAgent, env: TradingEnvironment, episode: int) -> Dict[str, float]:
        """Entra√Æne un √©pisode complet"""
        state = env.reset()
        done = False
        episode_reward = 0.0
        episode_length = 0
        episode_losses = {'actor': [], 'critic': [], 'alpha': []}

        start_time = time.time()

        while not done:
            # S√©lection de l'action
            if agent.total_steps < agent.config.warmup_steps:
                # Phase de warmup: actions al√©atoires
                action = env.action_space.sample()
            else:
                # Phase d'apprentissage: policy apprise
                action = agent.select_action(state, evaluate=False)

            # Ex√©cuter l'action dans l'environnement
            next_state, reward, done, info = env.step(action)

            # Stocker la transition dans le replay buffer
            agent.replay_buffer.push(state, action, reward, next_state, done)

            # Mettre √† jour l'agent (apr√®s warmup)
            if agent.total_steps >= agent.config.warmup_steps:
                losses = agent.update()
                if losses:
                    episode_losses['actor'].append(losses.get('actor_loss', 0))
                    episode_losses['critic'].append(losses.get('critic_loss', 0))
                    episode_losses['alpha'].append(losses.get('alpha', 0))

            # Accumuler les m√©triques
            episode_reward += reward
            episode_length += 1
            state = next_state
            agent.total_steps += 1

        # Incr√©menter le compteur d'√©pisodes
        agent.episode_count += 1

        # Calculer les moyennes des pertes
        avg_actor_loss = np.mean(episode_losses['actor']) if episode_losses['actor'] else 0
        avg_critic_loss = np.mean(episode_losses['critic']) if episode_losses['critic'] else 0
        avg_alpha = np.mean(episode_losses['alpha']) if episode_losses['alpha'] else 0

        elapsed_time = time.time() - start_time

        return {
            'reward': episode_reward,
            'length': episode_length,
            'actor_loss': avg_actor_loss,
            'critic_loss': avg_critic_loss,
            'alpha': avg_alpha,
            'time': elapsed_time,
            'final_equity': info.get('equity', 0),
            'total_return': info.get('total_return', 0),
            'sharpe': info.get('sharpe_ratio', 0),
            'max_drawdown': info.get('max_drawdown', 0),
        }

    def evaluate_agent(self, agent: SACAgent, env: TradingEnvironment, num_episodes: int = 3) -> Dict[str, float]:
        """√âvalue l'agent sur plusieurs √©pisodes"""
        eval_rewards = []
        eval_returns = []
        eval_sharpes = []
        eval_drawdowns = []

        for _ in range(num_episodes):
            state = env.reset()
            done = False
            episode_reward = 0.0

            while not done:
                # Mode √©valuation: pas d'exploration
                action = agent.select_action(state, evaluate=True)
                next_state, reward, done, info = env.step(action)
                episode_reward += reward
                state = next_state

            eval_rewards.append(episode_reward)
            eval_returns.append(info.get('total_return', 0))
            eval_sharpes.append(info.get('sharpe_ratio', 0))
            eval_drawdowns.append(info.get('max_drawdown', 0))

        return {
            'mean_reward': np.mean(eval_rewards),
            'std_reward': np.std(eval_rewards),
            'mean_return': np.mean(eval_returns),
            'mean_sharpe': np.mean(eval_sharpes),
            'mean_drawdown': np.mean(eval_drawdowns),
        }

    def save_checkpoint(self, agent: SACAgent, episode: int, metrics: Dict[str, Any]):
        """Sauvegarde un checkpoint"""
        checkpoint_path = self.checkpoints_dir / f"agent_{self.agent_id}_ep{episode}.pt"
        agent.save(str(checkpoint_path))

        # Sauvegarder aussi les m√©triques
        metrics_path = self.checkpoints_dir / f"metrics_ep{episode}.json"
        with open(metrics_path, 'w') as f:
            json.dump(metrics, f, indent=2)

        print(f"üíæ Checkpoint sauvegard√©: {checkpoint_path.name}")

    def save_training_stats(self):
        """Sauvegarde les statistiques d'entra√Ænement"""
        stats_path = self.logs_dir / "training_stats.json"
        with open(stats_path, 'w') as f:
            json.dump(self.training_stats, f, indent=2)

        # Sauvegarder aussi en format numpy pour analyse
        np.savez(
            self.logs_dir / "training_stats.npz",
            episode_rewards=np.array(self.training_stats['episode_rewards']),
            episode_lengths=np.array(self.training_stats['episode_lengths']),
            eval_rewards=np.array(self.training_stats['eval_rewards']),
            actor_losses=np.array(self.training_stats['actor_losses']),
            critic_losses=np.array(self.training_stats['critic_losses']),
            alpha_values=np.array(self.training_stats['alpha_values']),
        )

    def run_training(self):
        """Boucle principale d'entra√Ænement"""
        print("\n" + "="*70)
        print("üéØ D√âMARRAGE DE L'ENTRA√éNEMENT SAC SUR KAGGLE")
        print("="*70)

        # 1. Charger les donn√©es
        train_data, val_data, test_data = self.load_data()

        # 2. Calculer les features
        train_features, val_features, test_features = self.compute_features(
            train_data, val_data, test_data
        )

        # 3. Cr√©er les environnements
        train_env, val_env, env_config = self.create_environments(
            train_data, val_data, train_features, val_features
        )

        # 4. Cr√©er l'agent
        agent, sac_config = self.create_agent()

        # 5. Boucle d'entra√Ænement
        print("\n" + "="*70)
        print("üèãÔ∏è  ENTRA√éNEMENT EN COURS")
        print("="*70)

        best_eval_reward = -float('inf')

        for episode in range(1, self.num_episodes + 1):
            # Entra√Æner un √©pisode
            metrics = self.train_episode(agent, train_env, episode)

            # Logger les m√©triques
            self.training_stats['episode_rewards'].append(metrics['reward'])
            self.training_stats['episode_lengths'].append(metrics['length'])
            self.training_stats['actor_losses'].append(metrics['actor_loss'])
            self.training_stats['critic_losses'].append(metrics['critic_loss'])
            self.training_stats['alpha_values'].append(metrics['alpha'])

            # Afficher les r√©sultats
            print(f"\nüìà Episode {episode}/{self.num_episodes}")
            print(f"   Reward: {metrics['reward']:.2f}")
            print(f"   Length: {metrics['length']}")
            print(f"   Return: {metrics['total_return']:.2%}")
            print(f"   Sharpe: {metrics['sharpe']:.2f}")
            print(f"   MaxDD: {metrics['max_drawdown']:.2%}")
            print(f"   Actor Loss: {metrics['actor_loss']:.4f}")
            print(f"   Critic Loss: {metrics['critic_loss']:.4f}")
            print(f"   Alpha: {metrics['alpha']:.4f}")
            print(f"   Time: {metrics['time']:.1f}s")
            print(f"   Total Steps: {agent.total_steps}")

            # √âvaluation p√©riodique
            if episode % self.eval_frequency == 0:
                print(f"\nüîç √âvaluation (Episode {episode})...")
                eval_metrics = self.evaluate_agent(agent, val_env, num_episodes=3)
                self.training_stats['eval_rewards'].append(eval_metrics['mean_reward'])

                print(f"   Eval Reward: {eval_metrics['mean_reward']:.2f} ¬± {eval_metrics['std_reward']:.2f}")
                print(f"   Eval Return: {eval_metrics['mean_return']:.2%}")
                print(f"   Eval Sharpe: {eval_metrics['mean_sharpe']:.2f}")
                print(f"   Eval MaxDD: {eval_metrics['mean_drawdown']:.2%}")

                # Sauvegarder le meilleur mod√®le
                if eval_metrics['mean_reward'] > best_eval_reward:
                    best_eval_reward = eval_metrics['mean_reward']
                    best_model_path = self.output_dir / f"agent_{self.agent_id}_best.pt"
                    agent.save(str(best_model_path))
                    print(f"   ‚≠ê Nouveau meilleur mod√®le sauvegard√©!")

            # Sauvegarder des checkpoints p√©riodiques
            if episode % self.checkpoint_frequency == 0:
                self.save_checkpoint(agent, episode, metrics)

        # 6. Sauvegarder le mod√®le final
        print("\n" + "="*70)
        print("üíæ SAUVEGARDE DU MOD√àLE FINAL")
        print("="*70)

        final_model_path = self.output_dir / f"agent_{self.agent_id}_final.pt"
        agent.save(str(final_model_path))
        print(f"‚úÖ Mod√®le final sauvegard√©: {final_model_path}")

        # Sauvegarder les statistiques
        self.save_training_stats()
        print(f"‚úÖ Statistiques sauvegard√©es: {self.logs_dir}")

        # 7. R√©sum√© final
        print("\n" + "="*70)
        print("üéâ ENTRA√éNEMENT TERMIN√â")
        print("="*70)
        print(f"   Total √©pisodes: {self.num_episodes}")
        print(f"   Total steps: {agent.total_steps}")
        print(f"   Meilleure r√©compense eval: {best_eval_reward:.2f}")
        print(f"   R√©compense moyenne (10 derniers): {np.mean(self.training_stats['episode_rewards'][-10:]):.2f}")
        print(f"   Fichiers sauvegard√©s dans: {self.output_dir}")
        print("="*70)

        return agent, self.training_stats


def main():
    """Point d'entr√©e principal"""
    parser = argparse.ArgumentParser(description="Entra√Ænement SAC sur Kaggle")

    parser.add_argument(
        '--h5-path',
        type=str,
        default="/kaggle/input/trading-data/processed_data.h5",
        help="Chemin vers le fichier h5 avec les donn√©es"
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default="/kaggle/working",
        help="R√©pertoire de sortie"
    )
    parser.add_argument(
        '--num-episodes',
        type=int,
        default=100,
        help="Nombre d'√©pisodes d'entra√Ænement"
    )
    parser.add_argument(
        '--eval-frequency',
        type=int,
        default=10,
        help="Fr√©quence d'√©valuation (en √©pisodes)"
    )
    parser.add_argument(
        '--checkpoint-frequency',
        type=int,
        default=5,
        help="Fr√©quence de sauvegarde (en √©pisodes)"
    )
    parser.add_argument(
        '--agent-id',
        type=int,
        default=1,
        choices=[1, 2, 3],
        help="ID de l'agent (1, 2, ou 3)"
    )
    parser.add_argument(
        '--device',
        type=str,
        default="auto",
        choices=["auto", "cuda", "cpu"],
        help="Device PyTorch"
    )

    args = parser.parse_args()

    # Cr√©er le trainer
    trainer = KaggleTrainer(
        h5_path=args.h5_path,
        output_dir=args.output_dir,
        num_episodes=args.num_episodes,
        eval_frequency=args.eval_frequency,
        checkpoint_frequency=args.checkpoint_frequency,
        agent_id=args.agent_id,
        device=args.device
    )

    # Lancer l'entra√Ænement
    agent, stats = trainer.run_training()

    print("\n‚úÖ Script termin√© avec succ√®s!")


if __name__ == "__main__":
    main()
